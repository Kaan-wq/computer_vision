{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchmultimodal.diffusion_labs.modules.adapters.cfguidance import CFGuidance\n",
    "from torchmultimodal.diffusion_labs.modules.losses.diffusion_hybrid_loss import DiffusionHybridLoss\n",
    "from torchmultimodal.diffusion_labs.samplers.ddpm import DDPModule\n",
    "from torchmultimodal.diffusion_labs.predictors.noise_predictor import NoisePredictor\n",
    "from torchmultimodal.diffusion_labs.schedules.discrete_gaussian_schedule import linear_beta_schedule, DiscreteGaussianSchedule\n",
    "from torchmultimodal.diffusion_labs.transforms.diffusion_transform import RandomDiffusionSteps\n",
    "from torchmultimodal.diffusion_labs.utils.common import DiffusionOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = DiscreteGaussianSchedule(linear_beta_schedule(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = NoisePredictor(schedule, lambda x: torch.clamp(x, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, cond_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + cond_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pooling = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        _, _, w, h = x.size()\n",
    "        c = c.expand(-1, -1, w, h)\n",
    "        x = self.block(torch.cat([x, c], 1))\n",
    "        x_small = self.pooling(x)\n",
    "        return x, x_small\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, inp, out):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(inp*2, out, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out, out, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "    def forward(self, x, x_small):\n",
    "        x_big = self.upsample(x_small)\n",
    "        x = torch.cat((x_big, x), dim=1)\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, time_size=32, digit_size=32, steps=1000):\n",
    "        super().__init__()\n",
    "        cond_size = time_size + digit_size\n",
    "        self.conv = nn.Conv2d(1, 128, kernel_size=3, padding=1)\n",
    "        self.down = nn.ModuleList([DownBlock(128, 256, cond_size), DownBlock(256, 512, cond_size)])\n",
    "        self.bottleneck = DownBlock(512, 512, cond_size)\n",
    "        self.up = nn.ModuleList([UpBlock(512, 256), UpBlock(256, 128)])\n",
    "\n",
    "        self.variance = nn.Conv2d(128, 1, kernel_size=3, padding=1)\n",
    "        self.prediction = nn.Conv2d(128, 1, kernel_size=3, padding=1)\n",
    "        self.time_projection = nn.Embedding(steps, time_size)\n",
    "\n",
    "    def forward(self, x, t, conditional_inputs):\n",
    "        b, c, h, w = x.shape\n",
    "        timestep = self.time_projection(t).view(b, -1, 1, 1)\n",
    "        condition = conditional_inputs[\"context\"].view(b, -1, 1, 1)\n",
    "        condition = torch.cat([timestep, condition], dim=1)\n",
    "\n",
    "        x = self.conv(x)\n",
    "        self.outs = []\n",
    "        for block in self.down:\n",
    "            out, x = block(x, condition)\n",
    "            self.outs.append(out)\n",
    "        x, _ = self.bottleneck(x, condition)\n",
    "        for block in self.up:\n",
    "            x = block(self.outs.pop(), x)\n",
    "        v = self.variance(x)\n",
    "        p = self.prediction(x)\n",
    "        return DiffusionOutput(p, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(time_size=32, digit_size=32)\n",
    "unet = CFGuidance(unet, {\"context\": 32}, guidance=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_steps = torch.linspace(0, 999, 250, dtype=torch.long)\n",
    "model = DDPModule(unet, schedule, predictor, eval_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Embedding(10, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor, Lambda\n",
    "\n",
    "diffusion_transform = RandomDiffusionSteps(schedule, batched=False)\n",
    "transform = Compose([Resize(32),\n",
    "                     ToTensor(),\n",
    "                     Lambda(lambda x: 2*x - 1),\n",
    "                     Lambda(lambda x: diffusion_transform({\"x\": x}))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = FashionMNIST(\"fashion_mnist\", train=True, download=True, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=192, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "device = \"cpu\"\n",
    "encoder.to(device)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [{\"params\": encoder.parameters()}, {\"params\": model.parameters()}], lr=0.0001\n",
    ")\n",
    "h_loss = DiffusionHybridLoss(schedule)\n",
    "\n",
    "encoder.train()\n",
    "model.train()\n",
    "for e in range(epochs):\n",
    "\tfor sample in (pbar := tqdm(train_dataloader)):\n",
    "\t\tx, c = sample\n",
    "\t\tx0, xt, noise, t, c = x[\"x\"].to(device), x[\"xt\"].to(device), x[\"noise\"].to(device), x[\"t\"].to(device), c.to(device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tc = encoder(c)\n",
    "\t\tout = model(xt, t, {\"context\": c})\n",
    "\t\tloss = h_loss(out.prediction, noise, out.mean, out.log_variance, x0, xt, t)\n",
    "\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tpbar.set_description(f'{e+1}| Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_encoder(name, num=1):\n",
    "    fashion_dict = {\"t-shirt\": 0, \"pants\": 1, \"sweater\": 2, \"dress\": 3, \"coat\": 4, \n",
    "                    \"sandal\": 5, \"shirt\": 6, \"sneaker\": 7, \"purse\": 8, \"boot\": 9}\n",
    "    idx = torch.as_tensor([fashion_dict[name] for _ in range(num)]).to(device)\n",
    "\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        embed = encoder(idx)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "c = fashion_encoder(\"boot\", 9)\n",
    "noise = torch.randn(size=(9,1,32,32)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    imgs = model(noise, conditional_inputs={\"context\": c})\n",
    "\n",
    "img_grid = torchvision.utils.make_grid(imgs, 3)\n",
    "img = F.to_pil_image((img_grid + 1) / 2)\n",
    "img.resize((288, 288))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
